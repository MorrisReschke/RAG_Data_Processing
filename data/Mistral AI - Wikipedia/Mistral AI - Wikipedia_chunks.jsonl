{"id": "7fc171f0839d04be::c0", "text": "Mistral AI SAS (French: [mistʁal]) is a French artificial intelligence (AI) company, headquartered in Paris. Founded in 2023, it has open-weight large language models (LLMs), with both open-source and proprietary AI models. As of 2025 the company has a valuation of more than US$14 billion. The company is named after the mistral, a powerful, cold wind in southern France, a term which originates from the Occitan language. Mistral AI was established in April 2023 by three French AI researchers, Arthur Mensch, Guillaume Lample and Timothée Lacroix. Mensch, an expert in advanced AI systems, is a former employee of Google DeepMind; Lample and Lacroix, meanwhile, are large-scale AI models specialists who had worked for Meta Platforms. The trio originally met during their studies at École Polytechnique. In June 2023, the start-up carried out a first fundraising of €105 million ($117 million) with investors including the American fund Lightspeed Venture Partners, Eric Schmidt, Xavier Niel and JCDecaux. The valuation was then estimated by the Financial Times at €240 million ($267 million). On 10 December 2023, Mistral AI announced that it had raised €385 million ($428 million) as part of its second fundraising.", "metadata": {"headings": [{"heading": "Intro", "lvl": 2, "got_split": false}, {"heading": "Namesake", "lvl": 2, "got_split": false}, {"heading": "History", "lvl": 2, "got_split": false}, {"heading": "Funding", "lvl": 3, "got_split": true}], "chunk_index": 0, "word_count": 190, "start_char": 0, "end_char": 1220, "overlap_char": 0, "content_type": "charset=UTF-8", "content_hash": "37d3981af3468691b361ad3b75fe30d156164c6920e3cc6cf8d4efa0a1f147cc", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c1", "text": "The valuation was then estimated by the Financial Times at €240 million ($267 million). On 10 December 2023, Mistral AI announced that it had raised €385 million ($428 million) as part of its second fundraising. This round of financing involves the Californian fund Andreessen Horowitz, BNP Paribas and the software publisher Salesforce. By December 2023, it was valued at over $2 billion. On 16 April 2024, reporting revealed that Mistral was in talks to raise €500 million, a deal that would more than double its current valuation to at least €5 billion. In June 2024, Mistral AI secured a €600 million ($645 million) funding round, increasing its valuation to €5.8 billion ($6.2 billion). Based on valuation, as of June 2024, the company was ranked fourth globally in the AI industry, and first outside the San Francisco Bay Area. In August 2025, the Financial Times reported that Mistral was in talks to raise $1 billion at a $10 billion valuation. In September 2025, Bloomberg announced that Mistral AI has secured a €2 billion investment valuing it at €12 billion ($14 billion). This comes after $1.5 billion investment from Dutch company ASML, which owns 11% of Mistral.", "metadata": {"headings": [{"heading": "Funding", "lvl": 3, "got_split": true}], "chunk_index": 1, "word_count": 195, "start_char": 1009, "end_char": 2186, "overlap_char": 211, "content_type": "charset=UTF-8", "content_hash": "28c76b6245d8901320b17f034b9ed97c04b2b16c537df417133ca498fac773dd", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c2", "text": "In September 2025, Bloomberg announced that Mistral AI has secured a €2 billion investment valuing it at €12 billion ($14 billion). This comes after $1.5 billion investment from Dutch company ASML, which owns 11% of Mistral. On 26 February 2024, Microsoft announced that Mistral's language models would be made available on Microsoft's Azure cloud, while the multilingual conversational assistant Le Chat would be launched in the style of ChatGPT. The partnership also included a financial investment of $16 million by Microsoft in Mistral AI. In April 2025, Mistral AI announced a €100 million partnership with the shipping company CMA CGM. On 19 November, 2024, the company announced updates for Le Chat (pronounced /lə ʃa/ in French, like the French word for \"cat\"). It added the ability to create images, using Black Forest Labs' Flux Pro model. On 6 February 2025, Mistral AI released Le Chat on iOS and Android mobile devices. Mistral AI also introduced a Pro subscription tier, priced at $14.99 per month, which provides access to more advanced models, unlimited messaging, and web browsing.", "metadata": {"headings": [{"heading": "Partnerships", "lvl": 3, "got_split": false}, {"heading": "Services", "lvl": 2, "got_split": false}], "chunk_index": 2, "word_count": 176, "start_char": 1962, "end_char": 3060, "overlap_char": 224, "content_type": "charset=UTF-8", "content_hash": "c58e39b06678e93a0e87f0a4f3c3cdb8d7972444e2e6c4193864f59fa53ad494", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c3", "text": "On 6 February 2025, Mistral AI released Le Chat on iOS and Android mobile devices. Mistral AI also introduced a Pro subscription tier, priced at $14.99 per month, which provides access to more advanced models, unlimited messaging, and web browsing. The following table lists the main model versions of Mistral, describing the significant changes included with each version: Name: Voxtral Realtime; Release date: February 2026; Status: Active; Number of parameters (billion): 4; License: Apache-2.0; Notes: Realtime speech transcription. Name: Voxtral Mini Transcribe V2; Release date: February 2026; Status: Active; License: Proprietary; Notes: Speech understanding model. Name: Devstral Small 2; Release date: December 2025; Status: Active; Number of parameters (billion): 24; License: Apache-2.0; Notes: Compact, locally deployable coding model. Name: Devstral 2; Release date: December 2025; Status: Active; Number of parameters (billion): 123; License: Modified MIT license; Notes: Dense model. Name: Mistral Large 3; Release date: December 2025; Status: Active; Number of parameters (billion): 675 (41 active); License: Apache-2.0; Notes: A sparse mixture-of-experts models. Name: Ministral 3; Release date: December 2025; Status: Active; Number of parameters (billion): 3, 8 and 14; License: Apache-2.0; Notes: Three small, dense models with image understanding.", "metadata": {"headings": [{"heading": "Models", "lvl": 2, "got_split": true}], "chunk_index": 3, "word_count": 190, "start_char": 2812, "end_char": 4180, "overlap_char": 248, "content_type": "charset=UTF-8", "content_hash": "3b72ef0b30db6aa67f74fc83f9c82fa1f37cb9847261f7721f75b18f266cb786", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c4", "text": "Name: Mistral Large 3; Release date: December 2025; Status: Active; Number of parameters (billion): 675 (41 active); License: Apache-2.0; Notes: A sparse mixture-of-experts models. Name: Ministral 3; Release date: December 2025; Status: Active; Number of parameters (billion): 3, 8 and 14; License: Apache-2.0; Notes: Three small, dense models with image understanding. Name: Magistral Medium 1.2 25.09; Release date: September 2025; Status: Active; License: Proprietary; Notes: A refresh of Magistral Medium. Name: Magistral Small 1.2 25.09; Release date: September 2025; Status: Active; Number of parameters (billion): 24; License: Apache-2.0; Notes: A refresh of Magistral Small. Name: Mistral Medium 3.1 25.08; Release date: August 2025; Status: Active; License: Proprietary; Notes: A refresh of Mistral Medium 3, with improved tone and performance. Name: Codestral 25.08; Release date: August 2025; Status: Active; License: Proprietary; Notes: Code generation model. Name: Voxtral Small; Release date: July 2025; Status: Active; Number of parameters (billion): 24; License: Apache-2.0; Notes: Speech understanding model. Name: Voxtral Mini; Release date: July 2025; Status: Active; Number of parameters (billion): 3; License: Apache-2.0; Notes: Speech understanding model. Name: Devstral Medium 1.0; Release date: July 2025; Status: Active; License: Proprietary; Notes: Agentic coding model.", "metadata": {"headings": [{"heading": "Models", "lvl": 2, "got_split": true}], "chunk_index": 4, "word_count": 190, "start_char": 3811, "end_char": 5207, "overlap_char": 369, "content_type": "charset=UTF-8", "content_hash": "5d93f49f3c07f8e33709828be636e6df591ee1106b98cb7d185c81910dea3fba", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c5", "text": "Name: Voxtral Mini; Release date: July 2025; Status: Active; Number of parameters (billion): 3; License: Apache-2.0; Notes: Speech understanding model. Name: Devstral Medium 1.0; Release date: July 2025; Status: Active; License: Proprietary; Notes: Agentic coding model. Name: Devstral Small 1.1 25.07; Release date: July 2025; Status: Active; Number of parameters (billion): 24; License: Apache-2.0; Notes: Agentic coding model. Name: Mistral Small 3.2 25.06; Release date: June 2025; Status: Active; Number of parameters (billion): 24; License: Proprietary; Notes: A refresh of Mistral Small 3.1. Name: Magistral Medium; Release date: June 2025; License: Proprietary; Notes: Enterprise reasoning model. Name: Magistral Small; Release date: June 2025; Number of parameters (billion): 24; License: Apache-2.0; Notes: Open-weight reasoning model. Name: Devstral Small 25.05; Release date: May 2025; Status: Active; Number of parameters (billion): 24; License: Apache-2.0; Notes: Agentic model for software engineering tasks. Name: Mistral Medium 3 25.05; Release date: May 2025; License: Proprietary; Notes: Enterprise model available for on-premise deployment. Name: Mistral Small 3.1 25.03; Release date: March 2025; Number of parameters (billion): 24; License: Apache-2.0; Notes: A new leader in the small models category with image understanding capabilities, with the latest version v3.1 released March 2025.", "metadata": {"headings": [{"heading": "Models", "lvl": 2, "got_split": true}], "chunk_index": 5, "word_count": 193, "start_char": 4937, "end_char": 6349, "overlap_char": 270, "content_type": "charset=UTF-8", "content_hash": "5b24729fd8b605ee4e1549e6676edb7bfab898bc7341ec43a48b04ac138f556f", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c6", "text": "Name: Mistral Medium 3 25.05; Release date: May 2025; License: Proprietary; Notes: Enterprise model available for on-premise deployment. Name: Mistral Small 3.1 25.03; Release date: March 2025; Number of parameters (billion): 24; License: Apache-2.0; Notes: A new leader in the small models category with image understanding capabilities, with the latest version v3.1 released March 2025. Name: Mistral Small 3 25.01; Release date: January 2025; Number of parameters (billion): 24; License: Apache-2.0; Notes: Release in January 2025, Mistral Small 3 features 24B parameters. Name: Codestral 25.01; Release date: January 2025; License: Proprietary; Notes: Code generation model. Name: Mistral Large 2 24.11; Release date: November 2024; Number of parameters (billion): 123; License: Mistral Research License. Name: Pixtral Large 24.11; Release date: November 2024; Number of parameters (billion): 124; License: Mistral Research License; Notes: On November 19, 2024, the company introduced Pixtral Large, which integrates a 1-billion-parameter visual encoder coupled with Mistral Large 2. Name: Ministral 8B 24.10; Release date: October 2024; Number of parameters (billion): 8; License: Mistral Research License. Name: Ministral 3B 24.10; Release date: October 2024; Number of parameters (billion): 3; License: Proprietary. Name: Pixtral 24.09; Release date: September 2024; Number of parameters (billion): 12; License: Apache-2.0.", "metadata": {"headings": [{"heading": "Models", "lvl": 2, "got_split": true}], "chunk_index": 6, "word_count": 197, "start_char": 5961, "end_char": 7391, "overlap_char": 388, "content_type": "charset=UTF-8", "content_hash": "40fe2400d1c83e5e27a687bfe7d2361f477d1c191d2377fb9527d3d1e715ec2e", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c7", "text": "Name: Ministral 3B 24.10; Release date: October 2024; Number of parameters (billion): 3; License: Proprietary. Name: Pixtral 24.09; Release date: September 2024; Number of parameters (billion): 12; License: Apache-2.0. Name: Mistral Large 2 24.07; Release date: July 2024; Number of parameters (billion): 123; License: Mistral Research License; Notes: Mistral Large 2 was announced on July 24, 2024, and released on Hugging Face. It is available for free with a Mistral Research Licence, and with a commercial licence for commercial purposes. Mistral AI claims that it is fluent in dozens of languages, including many programming languages. Unlike the previous Mistral Large, this version was released with open weights. The model has 123 billion parameters and a context length of 128,000 tokens. Name: Codestral Mamba 7B; Release date: July 2024; Number of parameters (billion): 7; License: Apache-2.0; Notes: Codestral Mamba is based on the Mamba 2 architecture, which allows it to generate responses with longer input. Unlike Codestral, it was released under the Apache 2.0 license. While previous releases often included both the base model and the instruct version, only the instruct version of Codestral Mamba was released.", "metadata": {"headings": [{"heading": "Models", "lvl": 2, "got_split": true}], "chunk_index": 7, "word_count": 186, "start_char": 7173, "end_char": 8403, "overlap_char": 218, "content_type": "charset=UTF-8", "content_hash": "8ba1a11bf78a62ace0b8a961ab7f7c612758b0aa5992e1e9ee8b0bec5ede13ff", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c8", "text": "Unlike Codestral, it was released under the Apache 2.0 license. While previous releases often included both the base model and the instruct version, only the instruct version of Codestral Mamba was released. Name: Mathstral 7B; Release date: July 2024; Number of parameters (billion): 7; License: Apache-2.0; Notes: Mathstral 7B is a model with 7 billion parameters released by Mistral AI on July 16, 2024, focusing on STEM subjects. The model was produced in collaboration with Project Numina, and was released under the Apache 2.0 License with a context length of 32k tokens. Name: Codestral 22B; Release date: May 2024; Number of parameters (billion): 22; License: Mistral Non-Production License; Notes: Codestral is Mistral's first code-focused open weight model which was launched on May 29, 2024. Mistral claims Codestral is fluent in more than 80 programming languages Codestral has its own license which forbids the usage of Codestral for commercial purposes. Name: Mixtral 8x22B; Release date: April 2024; Number of parameters (billion): 141; License: Apache-2.0; Notes: Similar to Mistral's previous open models, Mixtral 8x22B was released via a BitTorrent link on Twitter on April 10, 2024, with a release on Hugging Face soon after.", "metadata": {"headings": [{"heading": "Models", "lvl": 2, "got_split": true}], "chunk_index": 8, "word_count": 192, "start_char": 8196, "end_char": 9440, "overlap_char": 207, "content_type": "charset=UTF-8", "content_hash": "f3cf8d8cdda47d12ee1710fc184d5e12389766656f16dc91b41d7d2260447063", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c9", "text": "Mistral claims Codestral is fluent in more than 80 programming languages Codestral has its own license which forbids the usage of Codestral for commercial purposes. Name: Mixtral 8x22B; Release date: April 2024; Number of parameters (billion): 141; License: Apache-2.0; Notes: Similar to Mistral's previous open models, Mixtral 8x22B was released via a BitTorrent link on Twitter on April 10, 2024, with a release on Hugging Face soon after. The model uses an architecture similar to that of Mistral 8x7B, but with each expert having 22 billion parameters instead of 7. In total, the model contains 141 billion parameters, as some parameters are shared among the experts, but offering higher performance. Name: Mistral Small; Release date: February 2024; License: Proprietary; Notes: Like the Large model, Mistral Small was launched on February 26, 2024. Name: Mistral Large 24.02; Release date: February 2024; License: Proprietary; Notes: Mistral Large was launched on February 26, 2024. It outputs in English, French, Spanish, German, and Italian, and provides coding capabilities. It is available on Microsoft Azure. Name: Mistral Medium; Release date: December 2023; License: Proprietary; Notes: Mistral Medium is trained in various languages including English, French, Italian, German, Spanish.", "metadata": {"headings": [{"heading": "Models", "lvl": 2, "got_split": true}], "chunk_index": 9, "word_count": 193, "start_char": 8999, "end_char": 10298, "overlap_char": 441, "content_type": "charset=UTF-8", "content_hash": "43c520ad9380df41fec16dd508f5288495fff1e4dd9169bae247b39d6a7ce801", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c10", "text": "It is available on Microsoft Azure. Name: Mistral Medium; Release date: December 2023; License: Proprietary; Notes: Mistral Medium is trained in various languages including English, French, Italian, German, Spanish. The number of parameters, and architecture of Mistral Medium is not known as Mistral has not published public information about it. Name: Mixtral 8x7B; Release date: December 2023; Number of parameters (billion): 46.7; License: Apache-2.0; Notes: Much like Mistral's first model, Mixtral 8x7B was released via a BitTorrent link posted on Twitter on December 9, 2023, and later Hugging Face and a blog post were released two days later. Unlike the previous Mistral model, Mixtral 8x7B uses a sparse mixture of experts architecture. The model has 8 distinct groups of \"experts\", giving the model a total of 46.7B usable parameters. Each single token can only use 12.9B parameters, therefore giving the speed and cost that a 12.9B parameter model would incur. A version trained to follow instructions called “Mixtral 8x7B Instruct” is also offered. Name: Mistral 7B; Release date: September 2023; Number of parameters (billion): 7.3; License: Apache-2.0; Notes: Mistral 7B is a 7.3B parameter language model using the transformers architecture.", "metadata": {"headings": [{"heading": "Models", "lvl": 2, "got_split": true}], "chunk_index": 10, "word_count": 190, "start_char": 10083, "end_char": 11340, "overlap_char": 215, "content_type": "charset=UTF-8", "content_hash": "a59bb4dc873e83e2d34e8cdb3a198e783b91d639b522ae09d890274e93e4cfc6", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c11", "text": "A version trained to follow instructions called “Mixtral 8x7B Instruct” is also offered. Name: Mistral 7B; Release date: September 2023; Number of parameters (billion): 7.3; License: Apache-2.0; Notes: Mistral 7B is a 7.3B parameter language model using the transformers architecture. It was officially released on September 27, 2023, via a BitTorrent magnet link, and Hugging Face under the Apache 2.0 license. Both a base model and \"instruct\" model were released with the latter receiving additional tuning to follow chat-style prompts. The fine-tuned model is only intended for demonstration purposes, and does not have guardrails or moderation built-in. Mistral AI claimed in the Mistral 7B release blog post that the model outperforms LLaMA 2 13B on all benchmarks tested, and is on par with LLaMA 34B on many benchmarks tested, despite having only 7 billion parameters, a small size compared to its competitors. Mistral AI claimed in 2023 that its model beat both LLaMA 70B, and GPT-3.5 in most benchmarks.", "metadata": {"headings": [{"heading": "Models", "lvl": 2, "got_split": true}, {"heading": "Mistral 7B", "lvl": 3, "got_split": false}, {"heading": "Mixtral 8x7B", "lvl": 3, "got_split": true}], "chunk_index": 11, "word_count": 159, "start_char": 11056, "end_char": 12068, "overlap_char": 284, "content_type": "charset=UTF-8", "content_hash": "753d9c69c18a9685cf0b896b6a5206b28244251302ad4ab2a9d42ec2b16ed62e", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c12", "text": "Mistral AI claimed in the Mistral 7B release blog post that the model outperforms LLaMA 2 13B on all benchmarks tested, and is on par with LLaMA 34B on many benchmarks tested, despite having only 7 billion parameters, a small size compared to its competitors. Mistral AI claimed in 2023 that its model beat both LLaMA 70B, and GPT-3.5 in most benchmarks. In March 2024, research conducted by Patronus AI comparing performance of LLMs on a 100-question test with prompts to generate text from books protected under U.S. copyright law found that OpenAI 's GPT-4, Mixtral, Meta AI 's LLaMA-2, and Anthropic 's Claude 2 generated copyrighted text verbatim in 44%, 22%, 10%, and 8% of responses respectively. On 17 March 2025, Mistral released Mistral Small 3.1 as a smaller, more efficient model. On 7 May 2025, Mistral AI released Mistral Medium 3. On 10 June 2025, Mistral AI released their first AI reasoning models: Magistral Small (open-source), and Magistral Medium, models which are purported to have chain-of-thought capabilities.", "metadata": {"headings": [{"heading": "Mixtral 8x7B", "lvl": 3, "got_split": true}, {"heading": "Mistral Small 3.1", "lvl": 3, "got_split": false}, {"heading": "Mistral Medium 3", "lvl": 3, "got_split": false}, {"heading": "Magistral Small and Magistral Medium", "lvl": 3, "got_split": false}], "chunk_index": 12, "word_count": 169, "start_char": 11714, "end_char": 12748, "overlap_char": 354, "content_type": "charset=UTF-8", "content_hash": "7ac7b431114d38cf281d3bc6a5c2085a3112ea12eaf1d804e99a419f01043380", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
{"id": "7fc171f0839d04be::c13", "text": "On 7 May 2025, Mistral AI released Mistral Medium 3. On 10 June 2025, Mistral AI released their first AI reasoning models: Magistral Small (open-source), and Magistral Medium, models which are purported to have chain-of-thought capabilities. On 2 December 2025, Mistral AI released Mistral Large 3, a sparse, mixture-of-experts model with 41 billion active parameters and 675 billion total parameters, and Ministral 3, three small, dense models with 3 billion, 7 billion and 14 billion parameters. On 10 December 2025, Mistral AI released Devstral 2 and Devstral Small 2. Devstral Small 2, a 24B parameter model is claimed to achieve better performance at coding than Qwen 3 Coder Flash model which is a 30B parameter model. On 10 December 2025, Mistral released Mistral Vibe, a command line interface for AI-Assisted software development. This was initially released with the Devstral 2 and Devstral Small 2 models available. Reasoning model List of large language models Official website Mistral AI on GitHub", "metadata": {"headings": [{"heading": "Mistral Large 3 and Ministral 3", "lvl": 3, "got_split": false}, {"heading": "Devstral 2 and Devstral Small 2", "lvl": 3, "got_split": false}, {"heading": "Mistral Vibe", "lvl": 2, "got_split": false}, {"heading": "See also", "lvl": 2, "got_split": false}], "chunk_index": 13, "word_count": 159, "start_char": 12507, "end_char": 13517, "overlap_char": 241, "content_type": "charset=UTF-8", "content_hash": "a9a5ccefec923ac2d19f0250f529f9efb822d07589c913bf3ccdf18e9c9f5263", "canonical_url": "https://en.wikipedia.org/wiki/Mistral_AI", "title": "Mistral AI - Wikipedia", "site": "Wikipedia (en)", "domain": "en.wikipedia.org", "language": "en", "fetched_at": "2026-02-10T07:34:51.580833+00:00", "doc_id": "7fc171f0839d04be"}}
